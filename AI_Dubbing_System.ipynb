{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HamzaAlSamman/arabic-video-dubbing/blob/main/AI_Dubbing_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvcoCEv0_kfY"
      },
      "outputs": [],
      "source": [
        "#@title 0) Setup Project Folders + (Optional) Mount Drive { display-mode: \"form\" }\n",
        "\n",
        "use_google_drive = True #@param {type:\"boolean\"}\n",
        "drive_base_path = \"/content/gdrive/MyDrive\" #@param {type:\"string\"}  # Change if using Shared Drive\n",
        "drive_upload_folder = \"dub_project\" #@param {type:\"string\"}          # Can be nested like: Courses/MyCourse/dub_project\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Optional: mount Google Drive\n",
        "if use_google_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/gdrive\", force_remount=True)\n",
        "\n",
        "# Project folders (local Colab runtime)\n",
        "PROJECT_ROOT = Path(\"/content/dub_project\")\n",
        "DIRS = {\n",
        "    \"input\": PROJECT_ROOT / \"input\",\n",
        "    \"work\": PROJECT_ROOT / \"work\",\n",
        "    \"extract\": PROJECT_ROOT / \"work\" / \"00_extract\",\n",
        "    \"output\": PROJECT_ROOT / \"output\",\n",
        "    \"tmp\": PROJECT_ROOT / \"tmp\",\n",
        "    \"logs\": PROJECT_ROOT / \"logs\",\n",
        "}\n",
        "for p in DIRS.values():\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "drive_upload_path = None\n",
        "if use_google_drive:\n",
        "    drive_upload_path = Path(drive_base_path) / drive_upload_folder\n",
        "    drive_upload_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(\"Drive folder ready:\", drive_upload_path)\n",
        "\n",
        "print(\"\\nLocal project folders ready:\")\n",
        "for k, v in DIRS.items():\n",
        "    print(f\"- {k}: {v}\")\n",
        "\n",
        "# Check ffmpeg\n",
        "import subprocess\n",
        "res = subprocess.run([\"ffmpeg\", \"-version\"], capture_output=True, text=True)\n",
        "print(\"\\nFFmpeg:\", res.stdout.splitlines()[0] if res.returncode == 0 else \"Not found\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1) Select video source + pick file_id { display-mode: \"form\" }\n",
        "\n",
        "choose = \"already_uploaded\" #@param [\"already_uploaded\",\"upload_now\",\"custom_path\"]\n",
        "custom_video_folder = \"\" #@param {type:\"string\"}  # Full path if choose=custom_path (Drive or /content path)\n",
        "\n",
        "video_extensions = (\".mp4\", \".mov\", \".mkv\", \".avi\", \".webm\")\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Pull from globals (in case user re-runs cells out of order)\n",
        "PROJECT_ROOT = globals().get(\"PROJECT_ROOT\", Path(\"/content/dub_project\"))\n",
        "DIRS = globals().get(\"DIRS\", {\"input\": PROJECT_ROOT / \"input\"})\n",
        "use_google_drive = globals().get(\"use_google_drive\", False)\n",
        "drive_base_path = globals().get(\"drive_base_path\", \"/content/gdrive/MyDrive\")\n",
        "drive_upload_folder = globals().get(\"drive_upload_folder\", \"dub_project\")\n",
        "\n",
        "# Resolve video_folder\n",
        "if choose == \"upload_now\":\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    input_dir = Path(DIRS[\"input\"])\n",
        "    input_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for fn in uploaded.keys():\n",
        "        src = Path(\"/content\") / fn\n",
        "        dst = input_dir / fn\n",
        "        os.replace(str(src), str(dst))\n",
        "\n",
        "    video_folder = str(input_dir)\n",
        "\n",
        "elif choose == \"custom_path\":\n",
        "    video_folder = (custom_video_folder or \"\").strip()\n",
        "    if not video_folder:\n",
        "        video_folder = str(DIRS[\"input\"])\n",
        "\n",
        "else:\n",
        "    # already_uploaded\n",
        "    # If Drive is mounted and base exists -> use drive folder, otherwise fallback to local input\n",
        "    if use_google_drive and Path(drive_base_path).exists():\n",
        "        video_folder = str(Path(drive_base_path) / drive_upload_folder)\n",
        "    else:\n",
        "        video_folder = str(DIRS[\"input\"])\n",
        "\n",
        "# Ensure folder exists\n",
        "Path(video_folder).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# List files\n",
        "ids, names = [], []\n",
        "id_monitor = {}\n",
        "video_id = 1\n",
        "\n",
        "for f in sorted(os.listdir(video_folder)):\n",
        "    if f.lower().endswith(video_extensions):\n",
        "        ids.append(video_id)\n",
        "        names.append(f)\n",
        "        id_monitor[video_id] = f\n",
        "        video_id += 1\n",
        "\n",
        "df = pd.DataFrame({\"file_name\": names, \"file_id\": ids}).set_index(\"file_id\")\n",
        "\n",
        "print(\"Folder:\", video_folder)\n",
        "if len(df):\n",
        "    print(\"\\nAvailable videos:\")\n",
        "    print(df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CTAyHoYzAnH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.1) Enter input File ID { display-mode: \"form\" }\n",
        "\n",
        "file_id = 3 #@param {type:\"number\"}\n",
        "\n",
        "import os\n",
        "\n",
        "selected_name = id_monitor.get(int(file_id), None)\n",
        "\n",
        "if selected_name is None:\n",
        "    raise ValueError(\"Invalid file_id. Choose a valid ID from the table above.\")\n",
        "\n",
        "INPUT_VIDEO = os.path.join(video_folder, selected_name)\n",
        "print(\"Selected video:\", INPUT_VIDEO)\n"
      ],
      "metadata": {
        "id": "iwQ3LZmfB7Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1.2) Separate audio from video using FFmpeg { display-mode: \"form\" }\n",
        "\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "def run_cmd(cmd):\n",
        "    print(\"Running:\\n\", \" \".join(map(str, cmd)))\n",
        "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if p.returncode != 0:\n",
        "        print(\"STDERR (tail):\\n\", (p.stderr or \"\")[-2000:])\n",
        "        raise RuntimeError(\"Command failed.\")\n",
        "    return p\n",
        "\n",
        "def has_audio_stream(video_path: Path) -> bool:\n",
        "    # Use ffprobe to detect audio stream\n",
        "    cmd = [\n",
        "        \"ffprobe\", \"-v\", \"error\",\n",
        "        \"-select_streams\", \"a:0\",\n",
        "        \"-show_entries\", \"stream=codec_type\",\n",
        "        \"-of\", \"default=nw=1:nk=1\",\n",
        "        str(video_path)\n",
        "    ]\n",
        "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    return (p.returncode == 0) and (\"audio\" in (p.stdout or \"\").strip().lower())\n",
        "\n",
        "def separate_audio_video(input_video_path: str, out_dir: str):\n",
        "    input_video_path = Path(input_video_path)\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if not input_video_path.exists():\n",
        "        raise FileNotFoundError(f\"Input video not found: {input_video_path}\")\n",
        "\n",
        "    base = input_video_path.stem\n",
        "    video_no_audio = out_dir / f\"{base}_no_audio.mp4\"\n",
        "    audio_raw_wav  = out_dir / f\"{base}_raw.wav\"\n",
        "    audio_16k_mono = out_dir / f\"{base}_16k_mono.wav\"\n",
        "\n",
        "    # Video without audio\n",
        "    run_cmd([\n",
        "        \"ffmpeg\", \"-y\",\n",
        "        \"-i\", str(input_video_path),\n",
        "        \"-map\", \"0:v:0\",\n",
        "        \"-c:v\", \"copy\",\n",
        "        \"-an\",\n",
        "        str(video_no_audio)\n",
        "    ])\n",
        "\n",
        "    # Audio extraction (only if audio exists)\n",
        "    if not has_audio_stream(input_video_path):\n",
        "        raise RuntimeError(\"No audio stream found in this video. Cannot continue ASR/dubbing pipeline.\")\n",
        "\n",
        "    # Extract raw audio (stereo 44.1k wav)\n",
        "    run_cmd([\n",
        "        \"ffmpeg\", \"-y\",\n",
        "        \"-i\", str(input_video_path),\n",
        "        \"-map\", \"0:a:0\",\n",
        "        \"-vn\",\n",
        "        \"-acodec\", \"pcm_s16le\",\n",
        "        \"-ar\", \"44100\",\n",
        "        \"-ac\", \"2\",\n",
        "        str(audio_raw_wav)\n",
        "    ])\n",
        "\n",
        "    # ASR-friendly 16k mono wav\n",
        "    run_cmd([\n",
        "        \"ffmpeg\", \"-y\",\n",
        "        \"-i\", str(audio_raw_wav),\n",
        "        \"-acodec\", \"pcm_s16le\",\n",
        "        \"-ar\", \"16000\",\n",
        "        \"-ac\", \"1\",\n",
        "        str(audio_16k_mono)\n",
        "    ])\n",
        "\n",
        "    return {\n",
        "        \"video_no_audio\": str(video_no_audio),\n",
        "        \"audio_raw_wav\": str(audio_raw_wav),\n",
        "        \"audio_16k_mono\": str(audio_16k_mono),\n",
        "    }\n",
        "\n",
        "# Require Step 0 + Step 1\n",
        "try:\n",
        "    DIRS\n",
        "except NameError:\n",
        "    raise NameError(\"DIRS not found. Run Step 0 first.\")\n",
        "if \"INPUT_VIDEO\" not in globals() or not INPUT_VIDEO:\n",
        "    raise NameError(\"INPUT_VIDEO not found. Run Step 1 first.\")\n",
        "\n",
        "outputs = separate_audio_video(INPUT_VIDEO, str(DIRS[\"extract\"]))\n",
        "\n",
        "# Save stable paths for later steps\n",
        "step1_paths = dict(outputs)\n",
        "step1_paths[\"input_video\"] = str(INPUT_VIDEO)\n",
        "step1_paths[\"extract_dir\"] = str(DIRS[\"extract\"])\n",
        "\n",
        "print(\"\\nOutputs (step1_paths):\")\n",
        "for k, v in step1_paths.items():\n",
        "    print(f\"- {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "vDmSRQD2B9qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.0) Install Demucs { display-mode: \"form\" }\n",
        "\n",
        "!pip -q install -U demucs\n"
      ],
      "metadata": {
        "id": "tMky5ihlGBuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fix torchaudio save error (torchcodec) { display-mode: \"form\" }\n",
        "\n",
        "!pip -q install torchcodec\n"
      ],
      "metadata": {
        "id": "Rgnf1zLdaLqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.1) Demucs (robust): vocals only + 16k mono + save paths { display-mode: \"form\" }\n",
        "\n",
        "demucs_model = \"htdemucs\" #@param [\"htdemucs\",\"htdemucs_ft\",\"mdx_extra\",\"mdx_extra_q\",\"mdx\"]\n",
        "device = \"cuda\" #@param [\"cuda\",\"cpu\"]\n",
        "segments_try = \"4,2,1\" #@param {type:\"string\"}\n",
        "use_two_stems_vocals = True #@param {type:\"boolean\"}\n",
        "shifts = 0 #@param {type:\"integer\"}\n",
        "overlap = 0.25 #@param {type:\"number\"}\n",
        "\n",
        "import os, sys, subprocess\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "# Optional dependency for *_q models\n",
        "if demucs_model.endswith(\"_q\"):\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"diffq\"], check=False)\n",
        "\n",
        "def run_cmd(cmd):\n",
        "    print(\"Running:\\n\", \" \".join(map(str, cmd)))\n",
        "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if p.returncode != 0:\n",
        "        print(\"\\n--- STDOUT (tail) ---\\n\", (p.stdout or \"\")[-2000:])\n",
        "        print(\"\\n--- STDERR (tail) ---\\n\", (p.stderr or \"\")[-2000:])\n",
        "        raise RuntimeError(f\"Command failed (code={p.returncode}).\")\n",
        "    if p.stderr:\n",
        "        print((p.stderr)[-600:])\n",
        "    return p\n",
        "\n",
        "# Require Step 0\n",
        "if \"DIRS\" not in globals():\n",
        "    raise NameError(\"DIRS not found. Run Step 0 first.\")\n",
        "\n",
        "# Require Step 1\n",
        "if \"INPUT_VIDEO\" not in globals() or not INPUT_VIDEO:\n",
        "    raise NameError(\"INPUT_VIDEO not found. Run Step 1 first.\")\n",
        "\n",
        "input_video_path = Path(INPUT_VIDEO)\n",
        "base = input_video_path.stem\n",
        "\n",
        "# Resolve audio_raw from step1_paths first\n",
        "audio_raw = None\n",
        "if \"step1_paths\" in globals() and isinstance(step1_paths, dict):\n",
        "    audio_raw = Path(step1_paths.get(\"audio_raw_wav\", \"\"))\n",
        "elif \"outputs\" in globals() and isinstance(outputs, dict):\n",
        "    audio_raw = Path(outputs.get(\"audio_raw_wav\", \"\"))\n",
        "\n",
        "# Fallback: search in extract dir\n",
        "if (audio_raw is None) or (not audio_raw.exists()):\n",
        "    candidates = sorted(Path(DIRS[\"extract\"]).glob(\"*.wav\"))\n",
        "    # Prefer *_raw.wav if exists\n",
        "    raw_candidates = [p for p in candidates if p.name.endswith(\"_raw.wav\")]\n",
        "    audio_raw = raw_candidates[-1] if raw_candidates else (candidates[-1] if candidates else None)\n",
        "\n",
        "if audio_raw is None or not audio_raw.exists():\n",
        "    raise FileNotFoundError(\"Could not find extracted RAW audio. Run Step 1.2 first.\")\n",
        "\n",
        "print(\"Using extracted audio RAW:\", audio_raw)\n",
        "\n",
        "# Output folders\n",
        "demucs_out_root = Path(DIRS[\"work\"]) / \"01_demucs\"\n",
        "prep_root       = Path(DIRS[\"work\"]) / \"02_prep\"\n",
        "demucs_out_root.mkdir(parents=True, exist_ok=True)\n",
        "prep_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Parse segments list\n",
        "seg_list = []\n",
        "for x in segments_try.split(\",\"):\n",
        "    x = x.strip()\n",
        "    if x:\n",
        "        seg_list.append(int(x))\n",
        "if not seg_list:\n",
        "    seg_list = [4,2,1]\n",
        "\n",
        "def try_demucs(run_device: str):\n",
        "    for seg in seg_list:\n",
        "        cmd = [sys.executable, \"-m\", \"demucs\",\n",
        "               \"-n\", demucs_model,\n",
        "               \"-d\", run_device,\n",
        "               \"--jobs\", \"0\",\n",
        "               \"--segment\", str(seg),\n",
        "               \"--shifts\", str(shifts),\n",
        "               \"--overlap\", str(overlap),\n",
        "               \"-o\", str(demucs_out_root),\n",
        "               str(audio_raw)]\n",
        "        if use_two_stems_vocals:\n",
        "            cmd.insert(cmd.index(\"-o\"), \"--two-stems\")\n",
        "            cmd.insert(cmd.index(\"-o\"), \"vocals\")\n",
        "\n",
        "        print(f\"\\nAttempt Demucs: device={run_device}, segment={seg}, two_stems={use_two_stems_vocals}, shifts={shifts}\")\n",
        "        try:\n",
        "            run_cmd(cmd)\n",
        "            return True\n",
        "        except RuntimeError:\n",
        "            print(f\"Failed with segment={seg} on device={run_device}. Trying next...\")\n",
        "    return False\n",
        "\n",
        "# Device fallback logic\n",
        "real_device = device\n",
        "if real_device == \"cuda\" and not torch.cuda.is_available():\n",
        "    print(\"CUDA not available. Falling back to CPU.\")\n",
        "    real_device = \"cpu\"\n",
        "\n",
        "ok = try_demucs(real_device)\n",
        "if not ok and real_device == \"cuda\":\n",
        "    print(\"GPU attempts failed. Falling back to CPU...\")\n",
        "    ok = try_demucs(\"cpu\")\n",
        "if not ok:\n",
        "    raise RuntimeError(\"Demucs failed on all settings.\")\n",
        "\n",
        "# Locate stems\n",
        "vocals_candidates = sorted(demucs_out_root.glob(\"**/vocals.wav\"))\n",
        "if not vocals_candidates:\n",
        "    raise FileNotFoundError(\"vocals.wav not found after Demucs.\")\n",
        "VOCALS_WAV = vocals_candidates[-1]\n",
        "\n",
        "bg_candidates = []\n",
        "bg_candidates += sorted(demucs_out_root.glob(\"**/no_vocals.wav\"))\n",
        "bg_candidates += sorted(demucs_out_root.glob(\"**/instrumental.wav\"))\n",
        "BKG_WAV = bg_candidates[-1] if bg_candidates else None\n",
        "\n",
        "print(\"\\nVOCALS_WAV:\", VOCALS_WAV)\n",
        "print(\"BKG_WAV:\", BKG_WAV if BKG_WAV else \"Not found (optional)\")\n",
        "\n",
        "# Convert vocals to 16k mono wav for your denoiser model\n",
        "MODEL_INPUT_16K = prep_root / f\"{base}_vocals_16k_mono.wav\"\n",
        "run_cmd([\n",
        "    \"ffmpeg\", \"-y\",\n",
        "    \"-i\", str(VOCALS_WAV),\n",
        "    \"-ac\", \"1\",\n",
        "    \"-ar\", \"16000\",\n",
        "    \"-acodec\", \"pcm_s16le\",\n",
        "    str(MODEL_INPUT_16K)\n",
        "])\n",
        "\n",
        "# Save paths for next steps\n",
        "step2_paths = {\n",
        "    \"audio_raw\": str(audio_raw),\n",
        "    \"demucs_out_root\": str(demucs_out_root),\n",
        "    \"vocals_wav\": str(VOCALS_WAV),\n",
        "    \"background_wav\": str(BKG_WAV) if BKG_WAV else None,\n",
        "    \"model_input_16k\": str(MODEL_INPUT_16K),\n",
        "}\n",
        "\n",
        "print(\"\\nstep2_paths saved:\")\n",
        "for k, v in step2_paths.items():\n",
        "    print(f\"- {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "eeMWDm1aJxXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.2) Speech Noise Separation Model { display-mode: \"form\" }\n",
        "\n",
        "weights_filename = \"/content/gdrive/MyDrive/dub_project/best_model.pth\" #@param {type:\"string\"}\n",
        "segment_seconds = 2.0 #@param {type:\"number\"}\n",
        "overlap_ratio = 0.5 #@param {type:\"number\"}\n",
        "base_channels = 32 #@param {type:\"number\"}\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "import soundfile as sf\n",
        "\n",
        "# Require prior steps\n",
        "if \"DIRS\" not in globals():\n",
        "    raise NameError(\"DIRS not found. Run Step 0 first.\")\n",
        "if \"step2_paths\" not in globals() or not isinstance(step2_paths, dict):\n",
        "    raise NameError(\"step2_paths not found. Run Step 2.1 first.\")\n",
        "\n",
        "MODEL_INPUT_16K = Path(step2_paths[\"model_input_16k\"])\n",
        "if not MODEL_INPUT_16K.exists():\n",
        "    raise FileNotFoundError(f\"MODEL_INPUT_16K not found: {MODEL_INPUT_16K}\")\n",
        "\n",
        "device_t = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device_t)\n",
        "\n",
        "WEIGHTS_PATH = Path(weights_filename.strip())\n",
        "if not WEIGHTS_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Weights not found: {WEIGHTS_PATH}\")\n",
        "print(\"Weights:\", WEIGHTS_PATH)\n",
        "\n",
        "# Enforce correct channels\n",
        "if int(base_channels) != 32:\n",
        "    print(\"base_channels should be 32 to match trained weights. Forcing 32.\")\n",
        "    base_channels = 32\n",
        "\n",
        "def match_size(x, ref):\n",
        "    _, _, H, W = x.shape\n",
        "    _, _, Hr, Wr = ref.shape\n",
        "    if H > Hr: x = x[:, :, :Hr, :]\n",
        "    if W > Wr: x = x[:, :, :, :Wr]\n",
        "    if x.shape[2] < Hr or x.shape[3] < Wr:\n",
        "        pad_h = Hr - x.shape[2]\n",
        "        pad_w = Wr - x.shape[3]\n",
        "        x = F.pad(x, (0, pad_w, 0, pad_h))\n",
        "    return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.c1 = nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False)\n",
        "        self.n1 = nn.InstanceNorm2d(out_ch, affine=True)\n",
        "        self.c2 = nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False)\n",
        "        self.n2 = nn.InstanceNorm2d(out_ch, affine=True)\n",
        "        self.act = nn.LeakyReLU(0.2)\n",
        "        self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False) if in_ch != out_ch else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        i = x if self.skip is None else self.skip(x)\n",
        "        x = self.act(self.n1(self.c1(x)))\n",
        "        x = self.n2(self.c2(x))\n",
        "        return self.act(x + i)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1, bias=False),\n",
        "            nn.InstanceNorm2d(out_ch, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            ResBlock(out_ch, out_ch),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, skip_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1, bias=False)\n",
        "        self.norm = nn.InstanceNorm2d(out_ch, affine=True)\n",
        "        self.act = nn.LeakyReLU(0.2)\n",
        "        self.res = ResBlock(out_ch + skip_ch, out_ch)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.act(self.norm(self.up(x)))\n",
        "        x = match_size(x, skip)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        return self.res(x)\n",
        "\n",
        "class DualStreamResUNet(nn.Module):\n",
        "    def __init__(self, base=32):\n",
        "        super().__init__()\n",
        "        self.enc1 = ResBlock(1, base)\n",
        "        self.enc2 = Down(base, base*2)\n",
        "        self.enc3 = Down(base*2, base*4)\n",
        "        self.enc4 = Down(base*4, base*8)\n",
        "        self.bot = ResBlock(base*8, base*8)\n",
        "\n",
        "        self.up3s = Up(base*8, base*4, base*4)\n",
        "        self.up2s = Up(base*4, base*2, base*2)\n",
        "        self.up1s = Up(base*2, base, base)\n",
        "\n",
        "        self.up3n = Up(base*8, base*4, base*4)\n",
        "        self.up2n = Up(base*4, base*2, base*2)\n",
        "        self.up1n = Up(base*2, base, base)\n",
        "\n",
        "        self.speech_head = nn.Sequential(nn.Conv2d(base, 1, 1), nn.Sigmoid())\n",
        "        self.noise_head  = nn.Sequential(nn.Conv2d(base, 1, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x)\n",
        "        e2 = self.enc2(e1)\n",
        "        e3 = self.enc3(e2)\n",
        "        e4 = self.enc4(e3)\n",
        "        b = self.bot(e4)\n",
        "\n",
        "        xs = self.up3s(b, e3)\n",
        "        xs = self.up2s(xs, e2)\n",
        "        xs = self.up1s(xs, e1)\n",
        "\n",
        "        xn = self.up3n(b, e3)\n",
        "        xn = self.up2n(xn, e2)\n",
        "        xn = self.up1n(xn, e1)\n",
        "\n",
        "        return self.speech_head(xs), self.noise_head(xn)\n",
        "\n",
        "def load_weights(model, path: Path):\n",
        "    # Fallback for older torch versions that don't support weights_only\n",
        "    try:\n",
        "        sd = torch.load(str(path), map_location=\"cpu\", weights_only=True)\n",
        "    except TypeError:\n",
        "        sd = torch.load(str(path), map_location=\"cpu\")\n",
        "\n",
        "    if any(k.startswith(\"module.\") for k in sd.keys()):\n",
        "        sd = {k.replace(\"module.\", \"\"): v for k, v in sd.items()}\n",
        "\n",
        "    model.load_state_dict(sd, strict=True)\n",
        "    return model\n",
        "\n",
        "CONFIG = {\"SR\": 16000, \"SEG_SEC\": float(segment_seconds), \"N_FFT\": 512, \"HOP\": 128, \"WIN\": 512}\n",
        "STFT_WINDOW_CACHE = {}\n",
        "\n",
        "def stft_mag_phase(wav: torch.Tensor):\n",
        "    win_key = (wav.device.type, CONFIG[\"WIN\"])\n",
        "    if win_key not in STFT_WINDOW_CACHE:\n",
        "        STFT_WINDOW_CACHE[win_key] = torch.hann_window(CONFIG[\"WIN\"], device=wav.device)\n",
        "    S = torch.stft(\n",
        "        wav, n_fft=CONFIG[\"N_FFT\"], hop_length=CONFIG[\"HOP\"], win_length=CONFIG[\"WIN\"],\n",
        "        window=STFT_WINDOW_CACHE[win_key], return_complex=True\n",
        "    )\n",
        "    mag_log = torch.log1p(torch.abs(S))\n",
        "    phase = torch.angle(S)\n",
        "    return mag_log, phase\n",
        "\n",
        "def istft_from_logmag_phase(mag_log: torch.Tensor, phase: torch.Tensor, length: int):\n",
        "    win_key = (mag_log.device.type, CONFIG[\"WIN\"])\n",
        "    if win_key not in STFT_WINDOW_CACHE:\n",
        "        STFT_WINDOW_CACHE[win_key] = torch.hann_window(CONFIG[\"WIN\"], device=mag_log.device)\n",
        "    mag = torch.expm1(mag_log).clamp_min(0.0)\n",
        "    S = torch.polar(mag, phase)\n",
        "    wav = torch.istft(\n",
        "        S, n_fft=CONFIG[\"N_FFT\"], hop_length=CONFIG[\"HOP\"], win_length=CONFIG[\"WIN\"],\n",
        "        window=STFT_WINDOW_CACHE[win_key], length=length\n",
        "    )\n",
        "    return wav\n",
        "\n",
        "@torch.no_grad()\n",
        "def denoise_waveform(model, wav_1d: torch.Tensor):\n",
        "    seg_len = int(CONFIG[\"SR\"] * CONFIG[\"SEG_SEC\"])\n",
        "    hop = int(seg_len * (1.0 - float(overlap_ratio)))\n",
        "    hop = max(1, hop)\n",
        "\n",
        "    T0 = wav_1d.numel()\n",
        "    wav = wav_1d\n",
        "\n",
        "    if wav.numel() < seg_len:\n",
        "        wav = F.pad(wav, (0, seg_len - wav.numel()))\n",
        "    else:\n",
        "        remainder = (wav.numel() - seg_len) % hop\n",
        "        if remainder != 0:\n",
        "            wav = F.pad(wav, (0, hop - remainder))\n",
        "\n",
        "    T = wav.numel()\n",
        "    win = torch.hann_window(seg_len, periodic=True)\n",
        "    win = win / (win.max() + 1e-8)\n",
        "\n",
        "    out = torch.zeros(T, dtype=torch.float32)\n",
        "    wsum = torch.zeros(T, dtype=torch.float32)\n",
        "\n",
        "    for start in range(0, T - seg_len + 1, hop):\n",
        "        chunk = wav[start:start+seg_len].unsqueeze(0).to(device_t)\n",
        "        mag_log, phase = stft_mag_phase(chunk)\n",
        "        Ms, _ = model(mag_log.unsqueeze(1))\n",
        "        mag_hat_log = torch.log1p(torch.expm1(mag_log) * Ms.squeeze(1))\n",
        "        pred = istft_from_logmag_phase(mag_hat_log, phase, length=seg_len).squeeze(0).cpu()\n",
        "        out[start:start+seg_len] += pred * win\n",
        "        wsum[start:start+seg_len] += win\n",
        "\n",
        "    out = out / (wsum + 1e-8)\n",
        "    return out[:T0]\n",
        "\n",
        "model = DualStreamResUNet(base=int(base_channels)).to(device_t).eval()\n",
        "model = load_weights(model, WEIGHTS_PATH)\n",
        "\n",
        "wav, sr = torchaudio.load(str(MODEL_INPUT_16K))\n",
        "wav = wav.mean(dim=0)\n",
        "if sr != CONFIG[\"SR\"]:\n",
        "    wav = torchaudio.functional.resample(wav, sr, CONFIG[\"SR\"])\n",
        "wav = wav.contiguous().float().cpu()\n",
        "\n",
        "clean = denoise_waveform(model, wav)\n",
        "\n",
        "denoise_dir = Path(DIRS[\"work\"]) / \"02_denoise\"\n",
        "denoise_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "video_stem = Path(INPUT_VIDEO).stem\n",
        "CLEAN_AUDIO_FOR_ASR = denoise_dir / f\"{video_stem}_speech_clean_16k.wav\"\n",
        "sf.write(str(CLEAN_AUDIO_FOR_ASR), clean.numpy(), CONFIG[\"SR\"], subtype=\"PCM_16\")\n",
        "\n",
        "print(\"CLEAN_AUDIO_FOR_ASR =\", CLEAN_AUDIO_FOR_ASR)\n",
        "\n",
        "step2_paths[\"speech_clean_16k\"] = str(CLEAN_AUDIO_FOR_ASR)\n"
      ],
      "metadata": {
        "id": "uK90wNkHjluW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.21) Sanity check (after 2.2) { display-mode: \"form\" }\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "if \"step2_paths\" not in globals():\n",
        "    raise NameError(\"step2_paths not found. Run Step 2.1 first.\")\n",
        "\n",
        "p = Path(step2_paths.get(\"speech_clean_16k\",\"\"))\n",
        "print(\"speech_clean_16k =\", p)\n",
        "print(\"Exists:\", p.exists())\n",
        "if p.exists():\n",
        "    print(\"Size:\", p.stat().st_size)\n"
      ],
      "metadata": {
        "id": "RQd_dZMmoD96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.25) Create/Repair WhisperX venv (NO torchvision) { display-mode: \"form\" }\n",
        "\n",
        "import sys, subprocess\n",
        "\n",
        "def sh(cmd):\n",
        "    print(\">>\", cmd)\n",
        "    subprocess.check_call(cmd, shell=True)\n",
        "\n",
        "def sh_try(cmd):\n",
        "    print(\">>\", cmd)\n",
        "    p = subprocess.run(cmd, shell=True, text=True, capture_output=True)\n",
        "    if p.returncode != 0:\n",
        "        print(\"STDERR tail:\\n\", (p.stderr or \"\")[-2500:])\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "sh(\"apt-get -y -qq update && apt-get -y -qq install ffmpeg libsndfile1\")\n",
        "sh(\"pip -q install -U uv\")\n",
        "\n",
        "sh(\"rm -rf /content/.venv\")\n",
        "sh(f\"uv venv /content/.venv -p {sys.executable}\")\n",
        "sh(\"uv pip install -p /content/.venv/bin/python -U pip setuptools wheel\")\n",
        "\n",
        "ok = sh_try(\n",
        "    \"uv pip install -p /content/.venv/bin/python --no-cache-dir \"\n",
        "    \"--index-url https://download.pytorch.org/whl/cu124 \"\n",
        "    \"torch torchaudio\"\n",
        ")\n",
        "if not ok:\n",
        "    ok = sh_try(\n",
        "        \"uv pip install -p /content/.venv/bin/python --no-cache-dir \"\n",
        "        \"--index-url https://download.pytorch.org/whl/cu121 \"\n",
        "        \"torch torchaudio\"\n",
        "    )\n",
        "if not ok:\n",
        "    sh(\n",
        "        \"uv pip install -p /content/.venv/bin/python --no-cache-dir \"\n",
        "        \"--index-url https://download.pytorch.org/whl/cpu \"\n",
        "        \"torch torchaudio\"\n",
        "    )\n",
        "\n",
        "sh(\"uv pip uninstall -p /content/.venv/bin/python -y torchvision || true\")\n",
        "\n",
        "sh(\"uv pip install -p /content/.venv/bin/python -U 'numpy<2'\")\n",
        "sh(\"uv pip install -p /content/.venv/bin/python -U 'transformers>=4.38,<5' 'accelerate>=0.30,<1' soundfile pysrt\")\n",
        "\n",
        "sh(\"uv pip install -p /content/.venv/bin/python -U git+https://github.com/m-bain/whisperX.git\")\n",
        "\n",
        "sh(\n",
        "    '/content/.venv/bin/python -c \"'\n",
        "    'import importlib.util as u; '\n",
        "    'import torch, torchaudio; '\n",
        "    'print(\\'torch:\\', torch.__version__, \\'cuda_available:\\', torch.cuda.is_available(), \\'cuda:\\', torch.version.cuda); '\n",
        "    'print(\\'torchaudio:\\', torchaudio.__version__); '\n",
        "    'print(\\'torchvision_spec:\\', u.find_spec(\\'torchvision\\')); '\n",
        "    'from transformers import Pipeline; print(\\'Pipeline: OK\\'); '\n",
        "    'import whisperx; print(\\'whisperx import: OK\\')\"'\n",
        ")\n",
        "\n",
        "print(\"venv ready: /content/.venv\")\n"
      ],
      "metadata": {
        "id": "XbBdi4c-pJQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.3) WhisperX ASR + Align -> original_en.srt (RUN AS SCRIPT) { display-mode: \"form\" }\n",
        "\n",
        "whisperx_model_name = \"large-v3\" #@param [\"small\",\"medium\",\"large-v2\",\"large-v3\"]\n",
        "language_code = \"en\" #@param {type:\"string\"}  # Use \"\" for auto-detect\n",
        "device_preference = \"cuda\" #@param [\"cuda\",\"cpu\"]\n",
        "compute_type_pref = \"float16\" #@param [\"float16\",\"int8\"]\n",
        "batch_size = 16 #@param {type:\"integer\"}\n",
        "chunk_size = 15 #@param {type:\"integer\"}\n",
        "\n",
        "from pathlib import Path\n",
        "import os, subprocess\n",
        "\n",
        "if \"DIRS\" not in globals():\n",
        "    raise NameError(\"DIRS not found. Run Step 0 first.\")\n",
        "if \"step2_paths\" not in globals() or not isinstance(step2_paths, dict):\n",
        "    raise NameError(\"step2_paths not found. Run Step 2.1/2.2 first.\")\n",
        "if \"INPUT_VIDEO\" not in globals() or not INPUT_VIDEO:\n",
        "    raise NameError(\"INPUT_VIDEO not found. Run Step 1 first.\")\n",
        "\n",
        "CLEAN_AUDIO_FOR_ASR = Path(step2_paths.get(\"speech_clean_16k\",\"\"))\n",
        "if not CLEAN_AUDIO_FOR_ASR.exists():\n",
        "    raise FileNotFoundError(\"speech_clean_16k not found. Run Step 2.2 first.\")\n",
        "\n",
        "asr_dir = Path(DIRS[\"work\"]) / \"03_asr\"\n",
        "asr_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "video_stem = Path(INPUT_VIDEO).stem\n",
        "OUT_SRT = asr_dir / f\"{video_stem}_original_en.srt\"\n",
        "if OUT_SRT.exists():\n",
        "    OUT_SRT.unlink()\n",
        "\n",
        "script_path = Path(\"/content/transcribe_whisperx.py\")\n",
        "script_path.write_text(r\"\"\"\n",
        "import os\n",
        "os.environ.setdefault(\"MPLBACKEND\", \"agg\")\n",
        "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
        "\n",
        "import argparse, gc\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import whisperx\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--audio\", required=True)\n",
        "    ap.add_argument(\"--out_srt\", required=True)\n",
        "    ap.add_argument(\"--model\", default=\"large-v3\")\n",
        "    ap.add_argument(\"--language\", default=\"\")\n",
        "    ap.add_argument(\"--device\", default=\"cuda\")\n",
        "    ap.add_argument(\"--compute_type\", default=\"float16\")\n",
        "    ap.add_argument(\"--batch_size\", type=int, default=16)\n",
        "    ap.add_argument(\"--chunk_size\", type=int, default=15)\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    audio_path = Path(args.audio)\n",
        "    out_srt = Path(args.out_srt)\n",
        "    out_dir = out_srt.parent\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if not audio_path.exists():\n",
        "        raise FileNotFoundError(f\"Audio not found: {audio_path}\")\n",
        "\n",
        "    device = \"cuda\" if (args.device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
        "    compute_type = args.compute_type\n",
        "\n",
        "    if device == \"cpu\" and compute_type.lower() in (\"float16\",\"fp16\"):\n",
        "        compute_type = \"int8\"\n",
        "\n",
        "    asr_options = {\n",
        "        \"initial_prompt\": \"Educational video. Use clear punctuation. Keep technical terms accurate.\",\n",
        "        \"suppress_numerals\": False,\n",
        "    }\n",
        "\n",
        "    kwargs = dict(\n",
        "        compute_type=compute_type,\n",
        "        asr_options=asr_options,\n",
        "        vad_method=\"silero\",\n",
        "    )\n",
        "    if args.language.strip():\n",
        "        kwargs[\"language\"] = args.language.strip()\n",
        "\n",
        "    print(\"Device:\", device, \"| compute_type:\", compute_type)\n",
        "    print(\"Loading model...\")\n",
        "    model = whisperx.load_model(args.model, device, **kwargs)\n",
        "\n",
        "    print(\"Loading audio...\")\n",
        "    audio = whisperx.load_audio(str(audio_path))\n",
        "\n",
        "    print(\"Transcribing...\")\n",
        "    result = model.transcribe(\n",
        "        audio,\n",
        "        batch_size=args.batch_size if device == \"cuda\" else max(1, min(4, args.batch_size)),\n",
        "        chunk_size=args.chunk_size,\n",
        "        print_progress=True,\n",
        "    )\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    lang = result.get(\"language\") or (args.language.strip() if args.language.strip() else None)\n",
        "    if not lang:\n",
        "        raise RuntimeError(\"No language detected/fixed.\")\n",
        "\n",
        "    print(\"Aligning... lang =\", lang)\n",
        "    align_model, metadata = whisperx.load_align_model(language_code=lang, device=device)\n",
        "    result_aligned = whisperx.align(\n",
        "        result[\"segments\"],\n",
        "        align_model,\n",
        "        metadata,\n",
        "        audio,\n",
        "        device,\n",
        "        return_char_alignments=False,\n",
        "        print_progress=False,\n",
        "    )\n",
        "\n",
        "    del align_model\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    from whisperx.utils import get_writer\n",
        "    writer = get_writer(\"srt\", output_dir=str(out_dir))\n",
        "\n",
        "    writer_options = {\"highlight_words\": False, \"max_line_width\": None, \"max_line_count\": None}\n",
        "\n",
        "    stem = out_srt.stem\n",
        "    dummy_audio_name = f\"{stem}.wav\"\n",
        "    writer({\"segments\": result_aligned[\"segments\"], \"language\": lang}, dummy_audio_name, writer_options)\n",
        "\n",
        "    generated = out_dir / f\"{stem}.srt\"\n",
        "    if not generated.exists():\n",
        "        raise FileNotFoundError(f\"Expected SRT not generated: {generated}\")\n",
        "\n",
        "    if generated.resolve() != out_srt.resolve():\n",
        "        generated.replace(out_srt)\n",
        "\n",
        "    print(\"Saved:\", out_srt)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\", encoding=\"utf-8\")\n",
        "\n",
        "cmd = [\n",
        "    \"/content/.venv/bin/python\",\n",
        "    str(script_path),\n",
        "    \"--audio\", str(CLEAN_AUDIO_FOR_ASR),\n",
        "    \"--out_srt\", str(OUT_SRT),\n",
        "    \"--model\", whisperx_model_name,\n",
        "    \"--language\", (language_code or \"\"),\n",
        "    \"--device\", device_preference,\n",
        "    \"--compute_type\", compute_type_pref,\n",
        "    \"--batch_size\", str(batch_size),\n",
        "    \"--chunk_size\", str(chunk_size),\n",
        "]\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"MPLBACKEND\"] = \"agg\"\n",
        "env[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "env[\"TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\"] = \"true\"\n",
        "\n",
        "p = subprocess.run(cmd, env=env, capture_output=True, text=True)\n",
        "print(p.stdout)\n",
        "\n",
        "if p.returncode != 0:\n",
        "    print(\"ERROR (stderr tail):\\n\", (p.stderr or \"\")[-4000:])\n",
        "    raise RuntimeError(f\"WhisperX script failed with exit code {p.returncode}\")\n",
        "\n",
        "if OUT_SRT.exists():\n",
        "    step2_paths[\"asr_srt_en\"] = str(OUT_SRT)\n",
        "    print(\"asr_srt_en =\", step2_paths[\"asr_srt_en\"])\n",
        "else:\n",
        "    raise FileNotFoundError(f\"SRT not found: {OUT_SRT}\")\n"
      ],
      "metadata": {
        "id": "icA7r3OypK2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.0) User Settings (Provider + API Key) - .env style {display-mode: \"form\"}\n",
        "\n",
        "import os, sys, subprocess\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "PROVIDER = \"groq\"  #@param [\"gemini\",\"groq\"]\n",
        "DOMAIN   = \"academic\"  #@param [\"general\",\"technical\",\"medical\",\"academic\"]\n",
        "\n",
        "GEMINI_MODEL = \"gemini-3-flash-preview\"  #@param {type:\"string\"}\n",
        "\n",
        "GROQ_MODEL = \"qwen/qwen3-32b\"  #@param [\"openai/gpt-oss-120b\",\"qwen/qwen3-32b\",\"custom\"]\n",
        "GROQ_MODEL_CUSTOM = \"\"  #@param {type:\"string\"}\n",
        "if GROQ_MODEL == \"custom\":\n",
        "    GROQ_MODEL = (GROQ_MODEL_CUSTOM or \"\").strip() or \"qwen/qwen3-32b\"\n",
        "\n",
        "# ---- Install deps once ----\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", \"python-dotenv\", \"groq\", \"google-genai\", \"google-generativeai\"])\n",
        "\n",
        "\n",
        "DOTENV_PATH = \"/content/.env\"\n",
        "p = Path(DOTENV_PATH)\n",
        "if not p.exists():\n",
        "    raise FileNotFoundError(f\".env not found at: {p}. Create it first (مثل المقال).\")\n",
        "\n",
        "load_dotenv(dotenv_path=str(p), override=False)\n",
        "\n",
        "def require_env(name: str) -> str:\n",
        "    v = os.environ.get(name)\n",
        "    if not v or not v.strip():\n",
        "        raise RuntimeError(f\"Missing {name}. Put it in .env then reload runtime/cell.\")\n",
        "    return v.strip()\n",
        "\n",
        "print(f\"Provider = {PROVIDER} | Domain = {DOMAIN}\")\n",
        "print(\"Gemini model:\", GEMINI_MODEL)\n",
        "print(\"Groq model  :\", GROQ_MODEL)\n",
        "\n",
        "# Validate only the provider you selected\n",
        "if PROVIDER == \"groq\":\n",
        "    require_env(\"GROQ_API_KEY\")\n",
        "    print(\"✅ GROQ_API_KEY loaded from .env (hidden).\")\n",
        "else:\n",
        "    require_env(\"GEMINI_API_KEY\")\n",
        "    print(\"✅ GEMINI_API_KEY loaded from .env (hidden).\")\n",
        "\n",
        "\n",
        "# ---- Install deps (مرة واحدة) ----\n",
        "pkgs = [\n",
        "    \"pysrt\",\n",
        "    \"groq\",\n",
        "    \"pydub\",\n",
        "    \"edge-tts\",\n",
        "    \"aiohttp\",\n",
        "    \"nest_asyncio\",\n",
        "    \"soundfile\",\n",
        "    # Gemini libs (نثبت الاثنين لضمان التوافق)\n",
        "    \"google-genai\",\n",
        "    \"google-generativeai\",\n",
        "]\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", *pkgs])\n",
        "\n",
        "print(\"✅ Dependencies installed/updated.\")\n"
      ],
      "metadata": {
        "id": "y_AxWc_1zYbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.1) Arabic dubbing rewrite for SRT (Time-aware + Auto-shorten) {display-mode:\"form\"}\n",
        "\n",
        "# ---- Tunables ----\n",
        "AR_CHARS_PER_SEC = 13.0   #@param {type:\"number\"}\n",
        "MIN_MAX_CHARS    = 12     #@param {type:\"integer\"}\n",
        "SOFT_OVER_BUDGET = 1.15   #@param {type:\"number\"}\n",
        "MAX_SPEEDUP_NEED = 1.8   #@param {type:\"number\"}\n",
        "MAX_COMPRESS_ROUNDS = 2   #@param {type:\"integer\"}\n",
        "\n",
        "MAX_CHARS_PER_CHUNK = 4500  #@param {type:\"integer\"}\n",
        "MAX_RETRIES = 6             #@param {type:\"integer\"}\n",
        "\n",
        "import os, re, json, time\n",
        "from pathlib import Path\n",
        "import pysrt\n",
        "\n",
        "# ---- Require previous steps ----\n",
        "try:\n",
        "    step2_paths\n",
        "    DIRS\n",
        "    INPUT_VIDEO\n",
        "except NameError:\n",
        "    raise NameError(\"Missing step2_paths/DIRS/INPUT_VIDEO. Run Step 0 + Step 2.3 + Step 3.0 first.\")\n",
        "\n",
        "SRC_SRT = Path(step2_paths.get(\"asr_srt_en\",\"\"))\n",
        "if not SRC_SRT.exists():\n",
        "    raise FileNotFoundError(f\"Missing asr_srt_en: {SRC_SRT}. Run Step 2.3 first.\")\n",
        "\n",
        "# ---- Output ----\n",
        "translate_dir = DIRS[\"work\"] / \"04_translate\"\n",
        "translate_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "video_stem = Path(INPUT_VIDEO).stem\n",
        "OUT_SRT = translate_dir / f\"{video_stem}_ar_dub.srt\"\n",
        "\n",
        "# ---- Provider vars from Step 3.0 ----\n",
        "PROVIDER = globals().get(\"PROVIDER\", \"gemini\")\n",
        "DOMAIN   = globals().get(\"DOMAIN\", \"general\")\n",
        "GEMINI_MODEL = globals().get(\"GEMINI_MODEL\", \"gemini-3-flash-preview\")\n",
        "GROQ_MODEL   = globals().get(\"GROQ_MODEL\", \"qwen/qwen3-32b\")\n",
        "\n",
        "# ---- Helpers ----\n",
        "def extract_json_array(text: str):\n",
        "    t = (text or \"\").replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "    lb = t.find(\"[\")\n",
        "    rb = t.rfind(\"]\")\n",
        "    if lb != -1 and rb != -1 and rb > lb:\n",
        "        cand = t[lb:rb+1].strip()\n",
        "        try:\n",
        "            return json.loads(cand)\n",
        "        except Exception:\n",
        "            pass\n",
        "    m = re.search(r\"\\[\\s*\\{.*\\}\\s*\\]\", t, flags=re.DOTALL)\n",
        "    if not m:\n",
        "        raise ValueError(\"No JSON array found in LLM output.\")\n",
        "    return json.loads(m.group(0))\n",
        "\n",
        "def call_groq(prompt: str) -> str:\n",
        "    from groq import Groq\n",
        "    key = os.getenv(\"GROQ_API_KEY\",\"\").strip()\n",
        "    if not key:\n",
        "        raise RuntimeError(\"Missing GROQ_API_KEY (set it in Step 3.0).\")\n",
        "\n",
        "    client = Groq(api_key=key)\n",
        "\n",
        "    base_kwargs = dict(\n",
        "        model=str(GROQ_MODEL),\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.25,\n",
        "        top_p=1,\n",
        "        stream=False,\n",
        "    )\n",
        "    try:\n",
        "        r = client.chat.completions.create(**base_kwargs, max_completion_tokens=6000)\n",
        "    except TypeError:\n",
        "        r = client.chat.completions.create(**base_kwargs, max_tokens=6000)\n",
        "\n",
        "    return r.choices[0].message.content\n",
        "\n",
        "def call_gemini(prompt: str) -> str:\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "    key = os.getenv(\"GEMINI_API_KEY\",\"\").strip()\n",
        "    if not key:\n",
        "        raise RuntimeError(\"Missing GEMINI_API_KEY (set it in Step 3.0).\")\n",
        "\n",
        "    client = genai.Client(api_key=key)\n",
        "    r = client.models.generate_content(\n",
        "        model=str(GEMINI_MODEL),\n",
        "        contents=prompt,\n",
        "        config=types.GenerateContentConfig(temperature=0.25),\n",
        "    )\n",
        "    return r.text or \"\"\n",
        "\n",
        "def llm(prompt: str) -> str:\n",
        "    return call_gemini(prompt) if PROVIDER == \"gemini\" else call_groq(prompt)\n",
        "\n",
        "def chunk_items(items, max_chars=4500):\n",
        "    chunks, cur, cur_len = [], [], 0\n",
        "    for it in items:\n",
        "        line = f'{it[\"i\"]}: dur={it[\"dur\"]}s max={it[\"max_chars\"]} | {it[\"text\"]}'\n",
        "        if cur and (cur_len + len(line) + 1 > max_chars):\n",
        "            chunks.append(cur)\n",
        "            cur, cur_len = [], 0\n",
        "        cur.append(it)\n",
        "        cur_len += len(line) + 1\n",
        "    if cur:\n",
        "        chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "def build_prompt_timeaware(chunk):\n",
        "    lines = \"\\n\".join([f'{x[\"i\"]}: dur={x[\"dur\"]}s max={x[\"max_chars\"]} | {x[\"text\"]}' for x in chunk])\n",
        "    return f\"\"\"\n",
        "أنت محرّر نصوص دبلجة محترف.\n",
        "\n",
        "المهمة:\n",
        "أعد صياغة كل سطر إلى العربية الفصحى الحديثة (MSA) بصياغة منطوقة مناسبة للدبلجة، مع الالتزام بزمن السطر.\n",
        "\n",
        "قواعد صارمة:\n",
        "- حافظ على المعنى دون إضافة أو حذف معلومات.\n",
        "- لا تستخدم العامية أو لهجات محكية.\n",
        "- التزم قدر الإمكان بحد \"max\" لعدد الأحرف في كل سطر.\n",
        "- حافظ على الأسماء والمصطلحات التقنية والأرقام بدقة.\n",
        "- لا تغيّر الأرقام داخل أسماء الإصدارات/الموديلات أو الرموز التقنية (مثل GPT-4, v2.1, USB 3.0).\n",
        "- اترك الأرقام كما هي (سيتم التعامل معها لاحقاً إذا لزم).\n",
        "- أخرج فقط JSON array بالشكل:\n",
        "  [{{\"i\":..,\"t\":\"..\"}}, ...]\n",
        "- كل \"i\" يجب أن يطابق نفس رقم السطر وبنفس الترتيب.\n",
        "\n",
        "المجال: {DOMAIN}\n",
        "\n",
        "المدخل:\n",
        "{lines}\n",
        "\"\"\".strip()\n",
        "\n",
        "def build_prompt_compress(chunk):\n",
        "    lines = \"\\n\".join([f'{x[\"i\"]}: max={x[\"max_chars\"]} | EN: {x[\"en\"]} | AR: {x[\"ar\"]}' for x in chunk])\n",
        "    return f\"\"\"\n",
        "أنت محرّر دبلجة محترف.\n",
        "\n",
        "المهمة:\n",
        "قصّر فقط النص العربي ليلائم حد \"max\" دون خسارة المعنى أو التفاصيل المهمة.\n",
        "\n",
        "قواعد صارمة:\n",
        "- لا تضف معلومات جديدة ولا تحذف نقاطًا أساسية.\n",
        "- أبقِ الأسماء والمصطلحات التقنية دقيقة.\n",
        "- العربية الفصحى الحديثة فقط (بدون عامية).\n",
        "- اترك الأرقام كما هي.\n",
        "- أخرج فقط JSON array:\n",
        "  [{{\"i\":..,\"t\":\"..\"}}, ...]\n",
        "- كل \"i\" يجب أن يطابق نفس رقم السطر وبنفس الترتيب.\n",
        "\n",
        "المجال: {DOMAIN}\n",
        "\n",
        "المدخل:\n",
        "{lines}\n",
        "\"\"\".strip()\n",
        "\n",
        "def translate_chunk_timeaware(chunk):\n",
        "    prompt = build_prompt_timeaware(chunk)\n",
        "    expected = [x[\"i\"] for x in chunk]\n",
        "    last_err = None\n",
        "\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            out = llm(prompt)\n",
        "            arr = extract_json_array(out)\n",
        "\n",
        "            got = [int(x[\"i\"]) for x in arr]\n",
        "            if len(arr) != len(chunk):\n",
        "                raise ValueError(\"Count mismatch\")\n",
        "            if got != expected:\n",
        "                raise ValueError(\"Index order mismatch\")\n",
        "\n",
        "            return {int(x[\"i\"]): str(x[\"t\"]).strip() for x in arr}\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            time.sleep(min(20, 2 ** attempt))\n",
        "\n",
        "    raise RuntimeError(f\"Translation failed: {last_err}\")\n",
        "\n",
        "def compress_chunk(chunk):\n",
        "    prompt = build_prompt_compress(chunk)\n",
        "    expected = [x[\"i\"] for x in chunk]\n",
        "    last_err = None\n",
        "\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            out = llm(prompt)\n",
        "            arr = extract_json_array(out)\n",
        "\n",
        "            got = [int(x[\"i\"]) for x in arr]\n",
        "            if len(arr) != len(chunk):\n",
        "                raise ValueError(\"Count mismatch\")\n",
        "            if got != expected:\n",
        "                raise ValueError(\"Index order mismatch\")\n",
        "\n",
        "            return {int(x[\"i\"]): str(x[\"t\"]).strip() for x in arr}\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            time.sleep(min(20, 2 ** attempt))\n",
        "\n",
        "    raise RuntimeError(f\"Compression failed: {last_err}\")\n",
        "\n",
        "# ---- Load SRT + budgets ----\n",
        "subs = pysrt.open(str(SRC_SRT), encoding=\"utf-8\")\n",
        "\n",
        "items = []\n",
        "for s in subs:\n",
        "    en = re.sub(r\"\\s*\\n\\s*\", \" \", (s.text or \"\").strip()).strip()\n",
        "    dur_sec = max(0.20, (s.end.ordinal - s.start.ordinal) / 1000.0)\n",
        "    max_chars = max(MIN_MAX_CHARS, int(dur_sec * AR_CHARS_PER_SEC))\n",
        "    items.append({\"i\": int(s.index), \"text\": en, \"dur\": round(dur_sec, 2), \"max_chars\": int(max_chars)})\n",
        "\n",
        "chunks = chunk_items(items, MAX_CHARS_PER_CHUNK)\n",
        "print(f\"Lines={len(items)} | Chunks={len(chunks)} | Provider={PROVIDER} | Domain={DOMAIN}\")\n",
        "print(\"Gemini:\", GEMINI_MODEL)\n",
        "print(\"Groq  :\", GROQ_MODEL)\n",
        "\n",
        "# ---- Pass 1 ----\n",
        "mapping = {}\n",
        "for ci, ch in enumerate(chunks, start=1):\n",
        "    mapping.update(translate_chunk_timeaware(ch))\n",
        "    print(f\"Pass1 chunk {ci}/{len(chunks)} done.\")\n",
        "\n",
        "# ---- Risk detection ----\n",
        "def est_need(ar_text: str, max_chars: int):\n",
        "    L = max(1, len((ar_text or \"\").strip()))\n",
        "    return L / max(1, int(max_chars))\n",
        "\n",
        "def find_bad(mapping_dict):\n",
        "    bad = []\n",
        "    for it in items:\n",
        "        i = it[\"i\"]\n",
        "        ar = (mapping_dict.get(i, \"\") or \"\").strip()\n",
        "        need = est_need(ar, it[\"max_chars\"])\n",
        "        if need > SOFT_OVER_BUDGET or need > MAX_SPEEDUP_NEED:\n",
        "            bad.append({\"i\": i, \"en\": it[\"text\"], \"ar\": ar, \"max_chars\": it[\"max_chars\"], \"need\": round(need, 2)})\n",
        "    bad.sort(key=lambda x: x[\"need\"], reverse=True)\n",
        "    return bad\n",
        "\n",
        "def chunk_bad(bad, max_chars=4500):\n",
        "    chunks, cur, cur_len = [], [], 0\n",
        "    for x in bad:\n",
        "        line = f'{x[\"i\"]}: max={x[\"max_chars\"]} | EN: {x[\"en\"]} | AR: {x[\"ar\"]}'\n",
        "        if cur and (cur_len + len(line) + 1 > max_chars):\n",
        "            chunks.append(cur); cur=[]; cur_len=0\n",
        "        cur.append(x); cur_len += len(line) + 1\n",
        "    if cur: chunks.append(cur)\n",
        "    return chunks\n",
        "\n",
        "# ---- Pass 2 (compress risky only) ----\n",
        "for r in range(1, MAX_COMPRESS_ROUNDS + 1):\n",
        "    bad = find_bad(mapping)\n",
        "    print(f\"Compression round {r}/{MAX_COMPRESS_ROUNDS} | risky={len(bad)}\")\n",
        "    if not bad:\n",
        "        break\n",
        "    for ci, ch in enumerate(chunk_bad(bad, MAX_CHARS_PER_CHUNK), start=1):\n",
        "        fixed = compress_chunk(ch)\n",
        "        mapping.update(fixed)\n",
        "        print(f\"Compress chunk {ci} done.\")\n",
        "\n",
        "# ---- Save final SRT ----\n",
        "for s in subs:\n",
        "    s.text = mapping.get(int(s.index), (s.text or \"\").strip())\n",
        "\n",
        "subs.save(str(OUT_SRT), encoding=\"utf-8\")\n",
        "step2_paths[\"srt_ar_dub\"] = str(OUT_SRT)\n",
        "\n",
        "print(\"✅ Saved:\", OUT_SRT)\n",
        "print(\"✅ step2_paths['srt_ar_dub'] =\", step2_paths[\"srt_ar_dub\"])\n"
      ],
      "metadata": {
        "id": "7gI7-NvXzi8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.2) Build TTS-SRT (convert standalone numbers to Arabic words) {display-mode:\"form\"}\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pysrt\n",
        "\n",
        "in_path = Path(step2_paths.get(\"srt_ar_dub\",\"\"))\n",
        "if not in_path.exists():\n",
        "    raise FileNotFoundError(f\"Missing srt_ar_dub: {in_path}\")\n",
        "\n",
        "out_path = in_path.with_name(in_path.stem + \"_tts_numbers.srt\")\n",
        "\n",
        "# --- digit normalization (Arabic-Indic + Persian) ---\n",
        "DIGIT_TRANS = str.maketrans(\"٠١٢٣٤٥٦٧٨٩۰۱۲۳۴۵۶۷۸۹\", \"01234567890123456789\")\n",
        "\n",
        "ONES = [\"صفر\",\"واحد\",\"اثنان\",\"ثلاثة\",\"أربعة\",\"خمسة\",\"ستة\",\"سبعة\",\"ثمانية\",\"تسعة\"]\n",
        "TENS = [\"\", \"عشرة\", \"عشرون\", \"ثلاثون\", \"أربعون\", \"خمسون\", \"ستون\", \"سبعون\", \"ثمانون\", \"تسعون\"]\n",
        "TEENS = {\n",
        "    11:\"أحد عشر\", 12:\"اثنا عشر\", 13:\"ثلاثة عشر\", 14:\"أربعة عشر\", 15:\"خمسة عشر\",\n",
        "    16:\"ستة عشر\", 17:\"سبعة عشر\", 18:\"ثمانية عشر\", 19:\"تسعة عشر\"\n",
        "}\n",
        "HUNDREDS = {\n",
        "    1:\"مئة\", 2:\"مئتان\", 3:\"ثلاثمئة\", 4:\"أربعمئة\", 5:\"خمسمئة\",\n",
        "    6:\"ستمئة\", 7:\"سبعمئة\", 8:\"ثمانمئة\", 9:\"تسعمئة\"\n",
        "}\n",
        "\n",
        "SCALES = [\n",
        "    (\"\", \"\", \"\", \"\"),  # units\n",
        "    (\"ألف\", \"ألفان\", \"آلاف\", \"ألف\"),            # 10^3\n",
        "    (\"مليون\", \"مليونان\", \"ملايين\", \"مليون\"),    # 10^6\n",
        "    (\"مليار\", \"ملياران\", \"مليارات\", \"مليار\"),   # 10^9\n",
        "    (\"تريليون\", \"تريليونان\", \"تريليونات\", \"تريليون\"),  # 10^12 (احتياط)\n",
        "]\n",
        "\n",
        "def two_digits(n: int) -> str:\n",
        "    if n < 10: return ONES[n]\n",
        "    if n == 10: return \"عشرة\"\n",
        "    if 11 <= n <= 19: return TEENS[n]\n",
        "    t, u = divmod(n, 10)\n",
        "    if u == 0: return TENS[t]\n",
        "    return f\"{ONES[u]} و{TENS[t]}\"\n",
        "\n",
        "def three_digits(n: int) -> str:\n",
        "    if n < 100: return two_digits(n)\n",
        "    h, r = divmod(n, 100)\n",
        "    htxt = HUNDREDS[h]\n",
        "    return htxt if r == 0 else f\"{htxt} و{two_digits(r)}\"\n",
        "\n",
        "def int_to_words(n: int) -> str:\n",
        "    if n == 0:\n",
        "        return \"صفر\"\n",
        "\n",
        "    parts = []\n",
        "    group_idx = 0\n",
        "    while n > 0:\n",
        "        n, group = divmod(n, 1000)\n",
        "        if group == 0:\n",
        "            group_idx += 1\n",
        "            continue\n",
        "\n",
        "        group_words = three_digits(group)\n",
        "\n",
        "        # scale forms\n",
        "        if group_idx >= len(SCALES):\n",
        "            # إذا رقم ضخم جدًا، نكمّل بنفس آخر Scale\n",
        "            singular, dual, plural, many = SCALES[-1][0], SCALES[-1][1], SCALES[-1][2], SCALES[-1][3]\n",
        "        else:\n",
        "            singular, dual, plural, many = SCALES[group_idx][0], SCALES[group_idx][1], SCALES[group_idx][2], SCALES[group_idx][3]\n",
        "\n",
        "        if group_idx == 0:\n",
        "            parts.append(group_words)\n",
        "        else:\n",
        "            if group == 1:\n",
        "                parts.append(singular)\n",
        "            elif group == 2:\n",
        "                parts.append(dual)\n",
        "            elif 3 <= group <= 10:\n",
        "                parts.append(f\"{group_words} {plural}\")\n",
        "            else:\n",
        "                parts.append(f\"{group_words} {many}\")\n",
        "\n",
        "        group_idx += 1\n",
        "\n",
        "    return \" و\".join(reversed(parts))\n",
        "\n",
        "def decimal_to_words(num_str: str) -> str:\n",
        "    s = num_str.replace(\",\", \"\").strip()\n",
        "    sign = \"\"\n",
        "    if s.startswith(\"+\"):\n",
        "        s = s[1:]\n",
        "    elif s.startswith(\"-\"):\n",
        "        sign = \"سالب \"\n",
        "        s = s[1:]\n",
        "\n",
        "    if \".\" in s:\n",
        "        ip, fp = s.split(\".\", 1)\n",
        "        ip = ip if ip else \"0\"\n",
        "        fp = fp.rstrip(\"0\")\n",
        "        base = int_to_words(int(ip))\n",
        "        if not fp:\n",
        "            return sign + base\n",
        "        frac = \" \".join(ONES[int(d)] for d in fp if d.isdigit())\n",
        "        return sign + f\"{base} فاصل {frac}\"\n",
        "\n",
        "    return sign + int_to_words(int(s))\n",
        "\n",
        "def should_skip_number(text: str, start: int, end: int) -> bool:\n",
        "    \"\"\"\n",
        "    Skip ONLY if number is part of LATIN technical token (GPT-4, v2.1, USB3.0, etc).\n",
        "    Important: DO NOT skip just because Arabic letters touch the number.\n",
        "    \"\"\"\n",
        "    prev = text[start-1] if start-1 >= 0 else \"\"\n",
        "    nxt  = text[end] if end < len(text) else \"\"\n",
        "    token = text[start:end]\n",
        "\n",
        "    # If adjacent to Latin letters => skip\n",
        "    if re.match(r\"[A-Za-z]\", prev) or re.match(r\"[A-Za-z]\", nxt):\n",
        "        return True\n",
        "\n",
        "    # If pattern like \"GPT-4\" or \"v2.1\" where previous is separator and char before it is Latin\n",
        "    if start-2 >= 0:\n",
        "        prev2 = text[start-2]\n",
        "        if re.match(r\"[A-Za-z]\", prev2) and prev in \"-_/\":\n",
        "            return True\n",
        "\n",
        "    # If previous word (within a small window) is a Latin acronym and token looks like version (has dot)\n",
        "    pre = text[max(0, start-12):start]\n",
        "    if \".\" in token and re.search(r\"[A-Za-z]{2,10}\\s*$\", pre):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Normalize Arabic thousands/decimal separators before regex\n",
        "def normalize_separators(s: str) -> str:\n",
        "    # Arabic thousands separator: '٬' (U+066C) -> ','\n",
        "    # Arabic decimal separator : '٫' (U+066B) -> '.'\n",
        "    return (s or \"\").replace(\"٬\", \",\").replace(\"٫\", \".\").replace(\"،\", \",\")\n",
        "\n",
        "# match standalone numbers with optional decimal and optional %\n",
        "num_re = re.compile(r\"[+-]?\\d[\\d,]*([.]\\d+)?%?\")\n",
        "\n",
        "def convert_numbers_in_text(s: str) -> str:\n",
        "    if not s:\n",
        "        return s\n",
        "\n",
        "    s2 = s.translate(DIGIT_TRANS)\n",
        "    s2 = normalize_separators(s2)\n",
        "\n",
        "    def repl(m):\n",
        "        raw = m.group(0)\n",
        "        st, en = m.start(), m.end()\n",
        "\n",
        "        if should_skip_number(s2, st, en):\n",
        "            return raw\n",
        "\n",
        "        is_pct = raw.endswith(\"%\")\n",
        "        core = raw[:-1] if is_pct else raw\n",
        "\n",
        "        try:\n",
        "            w = decimal_to_words(core)\n",
        "            return (w + \" بالمئة\") if is_pct else w\n",
        "        except Exception:\n",
        "            return raw\n",
        "\n",
        "    return num_re.sub(repl, s2)\n",
        "\n",
        "subs = pysrt.open(str(in_path), encoding=\"utf-8\")\n",
        "for sub in subs:\n",
        "    lines = sub.text.splitlines()\n",
        "    sub.text = \"\\n\".join(convert_numbers_in_text(ln) for ln in lines)\n",
        "\n",
        "subs.save(str(out_path), encoding=\"utf-8\")\n",
        "step2_paths[\"srt_ar_dub_tts_numbers\"] = str(out_path)\n",
        "\n",
        "print(\"Saved TTS SRT:\", out_path)\n",
        "print(\"✅ step2_paths['srt_ar_dub_tts_numbers'] =\", step2_paths[\"srt_ar_dub_tts_numbers\"])\n"
      ],
      "metadata": {
        "id": "of3e2JR3w81E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.0) TTS Provider Settings (Edge or ElevenLabs) - .env style {display-mode:\"form\"}\n",
        "\n",
        "import os, sys, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "TTS_PROVIDER = \"elevenlabs\"  #@param [\"edge\",\"elevenlabs\"]\n",
        "\n",
        "EDGE_VOICE = \"ar-AE-HamdanNeural\"  #@param [\"ar-AE-HamdanNeural\",\"ar-AE-FatimaNeural\",\"ar-SA-HamedNeural\",\"ar-SA-ZariyahNeural\",\"ar-EG-ShakirNeural\",\"ar-EG-SalmaNeural\"]\n",
        "EDGE_RATE_PERCENT = 0  #@param {type:\"integer\"}\n",
        "\n",
        "ELEVEN_MODEL_ID = \"eleven_multilingual_v2\"  #@param {type:\"string\"}\n",
        "ELEVEN_VOICE_ID = \"R6nda3uM038xEEKi7GFl\"    #@param {type:\"string\"}\n",
        "\n",
        "# load .env (مثل المقال)\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", \"python-dotenv\"])\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "DOTENV_PATH = \"/content/.env\"\n",
        "p = Path(DOTENV_PATH)\n",
        "if not p.exists():\n",
        "    raise FileNotFoundError(f\".env not found at: {p}\")\n",
        "load_dotenv(dotenv_path=str(p), override=False)\n",
        "\n",
        "def require_env(name: str) -> str:\n",
        "    v = os.environ.get(name)\n",
        "    if not v or not v.strip():\n",
        "        raise RuntimeError(f\"Missing {name}. Put it in .env.\")\n",
        "    return v.strip()\n",
        "\n",
        "if TTS_PROVIDER == \"elevenlabs\":\n",
        "    require_env(\"ELEVEN_API_KEY\")\n",
        "    print(\"✅ ELEVEN_API_KEY loaded from .env (hidden).\")\n",
        "\n",
        "print(\"TTS_PROVIDER =\", TTS_PROVIDER)\n"
      ],
      "metadata": {
        "id": "EB3Sf7DXs5_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.1) Build Dub Vocals from Arabic SRT (Edge OR ElevenLabs + Time Fit) { display-mode:\"form\" }\n",
        "\n",
        "# --- Timing/Audio settings ---\n",
        "max_speedup   = 1.8  #@param {type:\"number\"}   # quality sweet spot: 1.20~1.30\n",
        "vocal_gain_db = 3.0  #@param {type:\"number\"}\n",
        "target_sr     = 48000 #@param {type:\"integer\"}\n",
        "fade_ms       = 8    #@param {type:\"integer\"}  # 0 to disable\n",
        "\n",
        "import os, re, sys, asyncio, subprocess, time, hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import pysrt\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# ----------------------------\n",
        "# Preconditions\n",
        "# ----------------------------\n",
        "try:\n",
        "    DIRS\n",
        "    step2_paths\n",
        "    INPUT_VIDEO\n",
        "except NameError:\n",
        "    raise NameError(\"Run Step 0 + Step 2 + Step 3 first (need DIRS/step2_paths/INPUT_VIDEO).\")\n",
        "\n",
        "# ----------------------------\n",
        "# Read settings from Step 4.0 (NO overrides)\n",
        "# ----------------------------\n",
        "TTS_PROVIDER      = globals().get(\"TTS_PROVIDER\", \"edge\")\n",
        "EDGE_VOICE        = globals().get(\"EDGE_VOICE\", \"ar-SA-ZariyahNeural\")\n",
        "EDGE_RATE_PERCENT = int(globals().get(\"EDGE_RATE_PERCENT\", 0))\n",
        "\n",
        "ELEVEN_MODEL_ID   = globals().get(\"ELEVEN_MODEL_ID\", \"eleven_multilingual_v2\")\n",
        "ELEVEN_VOICE_ID   = globals().get(\"ELEVEN_VOICE_ID\", \"\")\n",
        "ELEVEN_OUTPUT_FORMAT = globals().get(\"ELEVEN_OUTPUT_FORMAT\", \"mp3_44100_128\")\n",
        "\n",
        "print(\"Using TTS_PROVIDER =\", TTS_PROVIDER)\n",
        "if TTS_PROVIDER == \"edge\":\n",
        "    print(\"Edge voice/rate    =\", EDGE_VOICE, f\"{EDGE_RATE_PERCENT:+d}%\")\n",
        "else:\n",
        "    print(\"Eleven model/voice =\", ELEVEN_MODEL_ID, ELEVEN_VOICE_ID)\n",
        "    print(\"Eleven output fmt  =\", ELEVEN_OUTPUT_FORMAT)\n",
        "\n",
        "# Ensure deps for selected provider\n",
        "if TTS_PROVIDER == \"edge\":\n",
        "    # edge-tts should already be installed in your Step 3.0 deps, but keep safe:\n",
        "    try:\n",
        "        import edge_tts  # noqa\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", \"edge-tts\", \"aiohttp\"])\n",
        "else:\n",
        "    try:\n",
        "        from elevenlabs.client import ElevenLabs  # noqa\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", \"elevenlabs\"])\n",
        "    if not os.getenv(\"ELEVEN_API_KEY\",\"\").strip():\n",
        "        import getpass\n",
        "        os.environ[\"ELEVEN_API_KEY\"] = getpass.getpass(\"Enter ELEVEN_API_KEY (hidden): \")\n",
        "\n",
        "# ----------------------------\n",
        "# Inputs\n",
        "# ----------------------------\n",
        "SRT_AR = Path(step2_paths.get(\"srt_ar_dub_tts_numbers\", step2_paths.get(\"srt_ar_dub\",\"\")))\n",
        "if not SRT_AR.exists():\n",
        "    raise FileNotFoundError(f\"Arabic dub SRT not found: {SRT_AR}\")\n",
        "\n",
        "ref_audio = step2_paths.get(\"audio_raw\") or step2_paths.get(\"audio_16k_mono\") or step2_paths.get(\"speech_clean_16k\")\n",
        "if not ref_audio:\n",
        "    raise RuntimeError(\"No reference audio path found in step2_paths.\")\n",
        "REF_AUDIO = Path(ref_audio)\n",
        "if not REF_AUDIO.exists():\n",
        "    raise FileNotFoundError(f\"Reference audio not found: {REF_AUDIO}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Output dirs\n",
        "# ----------------------------\n",
        "tts_dir = DIRS[\"work\"] / \"05_tts\"\n",
        "cache_dir = tts_dir / \"cache_tts\"\n",
        "tts_dir.mkdir(parents=True, exist_ok=True)\n",
        "cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "video_stem = Path(INPUT_VIDEO).stem\n",
        "DUB_VOCALS_WAV = tts_dir / f\"{video_stem}_dub_vocals_{target_sr}.wav\"\n",
        "\n",
        "# ----------------------------\n",
        "# Helpers\n",
        "# ----------------------------\n",
        "def run(cmd):\n",
        "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if p.returncode != 0:\n",
        "        print(\"CMD:\", \" \".join(map(str, cmd)))\n",
        "        print(\"STDERR tail:\\n\", (p.stderr or \"\")[-2500:])\n",
        "        raise RuntimeError(\"Command failed\")\n",
        "    return p\n",
        "\n",
        "def ms_from_pysrt_time(t):\n",
        "    return (t.hours*3600 + t.minutes*60 + t.seconds)*1000 + t.milliseconds\n",
        "\n",
        "def clean_text_ar(s: str) -> str:\n",
        "    s = (s or \"\").strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    for bad in [\"[\", \"]\", \"♫\", \"♪\"]:\n",
        "        s = s.replace(bad, \"\")\n",
        "    return s.strip()\n",
        "\n",
        "def stable_hash(s: str) -> str:\n",
        "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()[:16]\n",
        "\n",
        "def atempo_chain(speed: float) -> str:\n",
        "    \"\"\"\n",
        "    ffmpeg atempo supports 0.5..2.0 per filter; chain if outside range.\n",
        "    \"\"\"\n",
        "    if speed <= 0:\n",
        "        speed = 1.0\n",
        "    parts = []\n",
        "    remain = float(speed)\n",
        "\n",
        "    while remain > 2.0:\n",
        "        parts.append(2.0)\n",
        "        remain /= 2.0\n",
        "    while remain < 0.5:\n",
        "        parts.append(0.5)\n",
        "        remain /= 0.5\n",
        "    parts.append(remain)\n",
        "\n",
        "    parts = [max(0.5, min(2.0, float(x))) for x in parts]\n",
        "    return \",\".join([f\"atempo={x:.6f}\" for x in parts])\n",
        "\n",
        "async def tts_to_mp3(text: str, out_mp3: Path):\n",
        "    \"\"\"\n",
        "    Create MP3 using selected provider.\n",
        "    \"\"\"\n",
        "    if TTS_PROVIDER == \"edge\":\n",
        "        import edge_tts\n",
        "        import aiohttp\n",
        "        import asyncio as aio\n",
        "\n",
        "        rate = f\"{int(EDGE_RATE_PERCENT):+d}%\"\n",
        "        last_err = None\n",
        "\n",
        "        for attempt in range(1, 6):\n",
        "            try:\n",
        "                com = edge_tts.Communicate(text=text, voice=EDGE_VOICE, rate=rate)\n",
        "                await com.save(str(out_mp3))\n",
        "                return\n",
        "            except (aiohttp.client_exceptions.WSServerHandshakeError,\n",
        "                    aiohttp.client_exceptions.ClientConnectorError,\n",
        "                    aio.TimeoutError) as e:\n",
        "                last_err = e\n",
        "                await aio.sleep(min(8, 0.8 * attempt))\n",
        "\n",
        "        raise RuntimeError(f\"Edge TTS failed: {last_err}\")\n",
        "\n",
        "    else:\n",
        "        from elevenlabs.client import ElevenLabs\n",
        "\n",
        "        key = os.getenv(\"ELEVEN_API_KEY\",\"\").strip()\n",
        "        if not key:\n",
        "            raise RuntimeError(\"Missing ELEVEN_API_KEY\")\n",
        "\n",
        "        if not ELEVEN_VOICE_ID.strip():\n",
        "            raise RuntimeError(\"ELEVEN_VOICE_ID is empty. Set it in Step 4.0.\")\n",
        "\n",
        "        client = ElevenLabs(api_key=key)\n",
        "\n",
        "        last_err = None\n",
        "        for attempt in range(5):\n",
        "            try:\n",
        "                audio = client.text_to_speech.convert(\n",
        "                    text=text,\n",
        "                    voice_id=ELEVEN_VOICE_ID,\n",
        "                    model_id=ELEVEN_MODEL_ID,\n",
        "                    output_format=ELEVEN_OUTPUT_FORMAT,\n",
        "                )\n",
        "                data = audio if isinstance(audio, (bytes, bytearray)) else b\"\".join(audio)\n",
        "                out_mp3.write_bytes(data)\n",
        "                return\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "                time.sleep(0.8 * (attempt + 1))\n",
        "\n",
        "        raise RuntimeError(f\"ElevenLabs TTS failed: {last_err}\")\n",
        "\n",
        "def _get_loop():\n",
        "    try:\n",
        "        loop = asyncio.get_event_loop()\n",
        "        if loop.is_closed():\n",
        "            raise RuntimeError(\"closed loop\")\n",
        "        return loop\n",
        "    except Exception:\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "        return loop\n",
        "\n",
        "def tts_line_to_wav(text: str) -> Path:\n",
        "    \"\"\"\n",
        "    Generate MP3 via provider then convert to WAV (mono, target_sr).\n",
        "    Cached by provider + voice/model + rate + text.\n",
        "    \"\"\"\n",
        "    if TTS_PROVIDER == \"edge\":\n",
        "        meta = f\"edge|{EDGE_VOICE}|rate={EDGE_RATE_PERCENT}\"\n",
        "    else:\n",
        "        meta = f\"eleven|voice={ELEVEN_VOICE_ID}|model={ELEVEN_MODEL_ID}|fmt={ELEVEN_OUTPUT_FORMAT}\"\n",
        "\n",
        "    key = stable_hash(meta + \"|\" + text)\n",
        "\n",
        "    mp3_path = cache_dir / f\"{key}.mp3\"\n",
        "    wav_path = cache_dir / f\"{key}_{target_sr}.wav\"\n",
        "\n",
        "    if wav_path.exists() and wav_path.stat().st_size > 0:\n",
        "        return wav_path\n",
        "\n",
        "    if mp3_path.exists() and mp3_path.stat().st_size == 0:\n",
        "        mp3_path.unlink()\n",
        "\n",
        "    if (not mp3_path.exists()) or mp3_path.stat().st_size == 0:\n",
        "        loop = _get_loop()\n",
        "        loop.run_until_complete(tts_to_mp3(text, mp3_path))\n",
        "\n",
        "    run([\n",
        "        \"ffmpeg\",\"-y\",\"-loglevel\",\"error\",\n",
        "        \"-i\", str(mp3_path),\n",
        "        \"-ac\",\"1\",\n",
        "        \"-ar\", str(target_sr),\n",
        "        \"-acodec\",\"pcm_s16le\",\n",
        "        str(wav_path)\n",
        "    ])\n",
        "    return wav_path\n",
        "\n",
        "def fit_to_slot(wav_in: Path, slot_ms: int, out_wav: Path):\n",
        "    \"\"\"\n",
        "    If wav longer -> speed up (cap at max_speedup).\n",
        "    If wav shorter -> pad silence to reach slot.\n",
        "    \"\"\"\n",
        "    audio = AudioSegment.from_file(str(wav_in))\n",
        "    dur = len(audio)\n",
        "\n",
        "    if slot_ms <= 0:\n",
        "        AudioSegment.silent(duration=0).export(str(out_wav), format=\"wav\")\n",
        "        return out_wav\n",
        "\n",
        "    if dur <= 0:\n",
        "        AudioSegment.silent(duration=slot_ms).export(str(out_wav), format=\"wav\")\n",
        "        return out_wav\n",
        "\n",
        "    if dur > slot_ms:\n",
        "        speed = dur / max(1, slot_ms)\n",
        "        speed = min(float(speed), float(max_speedup))\n",
        "        filt = atempo_chain(speed)\n",
        "\n",
        "        run([\n",
        "            \"ffmpeg\",\"-y\",\"-loglevel\",\"error\",\n",
        "            \"-i\", str(wav_in),\n",
        "            \"-filter:a\", f\"{filt},aresample={target_sr}\",\n",
        "            \"-ac\",\"1\",\"-ar\", str(target_sr),\n",
        "            str(out_wav)\n",
        "        ])\n",
        "\n",
        "        a2 = AudioSegment.from_file(str(out_wav))\n",
        "        if len(a2) > slot_ms:\n",
        "            a2 = a2[:slot_ms]\n",
        "            a2.export(str(out_wav), format=\"wav\")\n",
        "        return out_wav\n",
        "\n",
        "    pad = slot_ms - dur\n",
        "    out = audio + AudioSegment.silent(duration=pad)\n",
        "    out.export(str(out_wav), format=\"wav\")\n",
        "    return out_wav\n",
        "\n",
        "# ----------------------------\n",
        "# Build timeline\n",
        "# ----------------------------\n",
        "ref_seg = AudioSegment.from_file(str(REF_AUDIO))\n",
        "total_ms = len(ref_seg)\n",
        "\n",
        "subs = pysrt.open(str(SRT_AR), encoding=\"utf-8\")\n",
        "timeline = AudioSegment.silent(duration=total_ms, frame_rate=target_sr)\n",
        "\n",
        "lines_done = 0\n",
        "total_lines = len(subs)\n",
        "\n",
        "for s in subs:\n",
        "    start_ms = ms_from_pysrt_time(s.start)\n",
        "    end_ms   = ms_from_pysrt_time(s.end)\n",
        "    slot_ms  = max(0, end_ms - start_ms)\n",
        "\n",
        "    text = clean_text_ar(s.text)\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    base_wav = tts_line_to_wav(text)\n",
        "\n",
        "    if TTS_PROVIDER == \"edge\":\n",
        "        meta_fit = f\"edge|{EDGE_VOICE}|rate={EDGE_RATE_PERCENT}|sr={target_sr}|maxspd={max_speedup}\"\n",
        "    else:\n",
        "        meta_fit = f\"eleven|{ELEVEN_VOICE_ID}|{ELEVEN_MODEL_ID}|{ELEVEN_OUTPUT_FORMAT}|sr={target_sr}|maxspd={max_speedup}\"\n",
        "\n",
        "    fit_key = stable_hash(meta_fit + f\"|slot={slot_ms}|\" + text)\n",
        "    fitted = cache_dir / f\"{fit_key}_fit.wav\"\n",
        "\n",
        "    if not fitted.exists() or fitted.stat().st_size == 0:\n",
        "        fit_to_slot(base_wav, slot_ms, fitted)\n",
        "\n",
        "    clip = AudioSegment.from_file(str(fitted))\n",
        "    if fade_ms and fade_ms > 0 and len(clip) > (fade_ms * 2):\n",
        "        clip = clip.fade_in(fade_ms).fade_out(fade_ms)\n",
        "\n",
        "    if start_ms < total_ms:\n",
        "        timeline = timeline.overlay(clip, position=max(0, start_ms))\n",
        "\n",
        "    lines_done += 1\n",
        "    if lines_done % 25 == 0:\n",
        "        print(f\"Processed lines: {lines_done}/{total_lines}\")\n",
        "\n",
        "timeline = timeline + float(vocal_gain_db)\n",
        "\n",
        "timeline.export(str(DUB_VOCALS_WAV), format=\"wav\")\n",
        "\n",
        "# keep your existing key for next steps\n",
        "if int(target_sr) == 48000:\n",
        "    step2_paths[\"dub_vocals_wav_48k\"] = str(DUB_VOCALS_WAV)\n",
        "else:\n",
        "    step2_paths[\"dub_vocals_wav_48k\"] = str(DUB_VOCALS_WAV)  # still keep same key for compatibility\n",
        "\n",
        "print(\"✅ Dub vocals saved:\", DUB_VOCALS_WAV)\n",
        "print(\"✅ step2_paths['dub_vocals_wav_48k'] =\", step2_paths[\"dub_vocals_wav_48k\"])\n"
      ],
      "metadata": {
        "id": "k07DaZIb-8p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.2) Mix Background + Dub Vocals (Sidechain Ducking + Loudnorm) { display-mode: \"form\" }\n",
        "\n",
        "music_gain_db = -2.0 #@param {type:\"number\"}      # reduce background a bit\n",
        "duck_threshold = 0.02 #@param {type:\"number\"}     # sidechain threshold\n",
        "duck_ratio = 8.0 #@param {type:\"number\"}          # stronger ducking\n",
        "duck_attack_ms = 20 #@param {type:\"integer\"}\n",
        "duck_release_ms = 250 #@param {type:\"integer\"}\n",
        "target_sr = 48000 #@param {type:\"integer\"}\n",
        "\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def run(cmd):\n",
        "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if p.returncode != 0:\n",
        "        print(\"CMD:\", \" \".join(map(str, cmd)))\n",
        "        print(\"STDERR tail:\\n\", (p.stderr or \"\")[-3000:])\n",
        "        raise RuntimeError(\"Command failed\")\n",
        "    return p\n",
        "\n",
        "# require dub vocals\n",
        "DUB_VOCALS = Path(step2_paths.get(\"dub_vocals_wav_48k\",\"\"))\n",
        "if not DUB_VOCALS.exists():\n",
        "    raise FileNotFoundError(\"Missing dub vocals. Run Step 4.1 first.\")\n",
        "\n",
        "mix_dir = DIRS[\"work\"] / \"06_mix\"\n",
        "mix_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "video_stem = Path(INPUT_VIDEO).stem\n",
        "BKG_PREP = mix_dir / f\"{video_stem}_background_{target_sr}_stereo.wav\"\n",
        "VOC_PREP = mix_dir / f\"{video_stem}_dubvoc_{target_sr}_stereo.wav\"\n",
        "FINAL_WAV = mix_dir / f\"{video_stem}_final_mix_{target_sr}.wav\"\n",
        "\n",
        "# locate background wav\n",
        "bkg = step2_paths.get(\"background_wav\")\n",
        "bkg_path = Path(bkg) if bkg else None\n",
        "\n",
        "if not bkg_path or not bkg_path.exists():\n",
        "    # try build from bass/drums/other if available\n",
        "    demucs_root = Path(step2_paths.get(\"demucs_out_root\",\"\"))\n",
        "    bass = next(iter(sorted(demucs_root.glob(\"**/bass.wav\"))), None) if demucs_root.exists() else None\n",
        "    drums = next(iter(sorted(demucs_root.glob(\"**/drums.wav\"))), None) if demucs_root.exists() else None\n",
        "    other = next(iter(sorted(demucs_root.glob(\"**/other.wav\"))), None) if demucs_root.exists() else None\n",
        "\n",
        "    if bass and drums and other:\n",
        "        tmp_bkg = mix_dir / f\"{video_stem}_background_built.wav\"\n",
        "        # mix the 3 stems\n",
        "        run([\n",
        "            \"ffmpeg\",\"-y\",\"-loglevel\",\"error\",\n",
        "            \"-i\", str(bass), \"-i\", str(drums), \"-i\", str(other),\n",
        "            \"-filter_complex\", \"amix=inputs=3:duration=longest:dropout_transition=0\",\n",
        "            str(tmp_bkg)\n",
        "        ])\n",
        "        bkg_path = tmp_bkg\n",
        "    else:\n",
        "        # fallback: use original extracted raw audio (still works but less clean)\n",
        "        raw = step2_paths.get(\"audio_raw\") or step2_paths.get(\"audio_16k_mono\")\n",
        "        if not raw:\n",
        "            raise RuntimeError(\"No background and no raw audio fallback found.\")\n",
        "        bkg_path = Path(raw)\n",
        "\n",
        "print(\"Background source:\", bkg_path)\n",
        "\n",
        "# prep background to stereo target_sr + gain\n",
        "run([\n",
        "    \"ffmpeg\",\"-y\",\"-loglevel\",\"error\",\n",
        "    \"-i\", str(bkg_path),\n",
        "    \"-filter:a\", f\"volume={music_gain_db}dB,aresample={target_sr},pan=stereo|c0=c0|c1=c0\",\n",
        "    str(BKG_PREP)\n",
        "])\n",
        "\n",
        "# prep vocals to stereo target_sr\n",
        "run([\n",
        "    \"ffmpeg\",\"-y\",\"-loglevel\",\"error\",\n",
        "    \"-i\", str(DUB_VOCALS),\n",
        "    \"-filter:a\", f\"aresample={target_sr},pan=stereo|c0=c0|c1=c0\",\n",
        "    str(VOC_PREP)\n",
        "])\n",
        "\n",
        "# mix with sidechain ducking + loudnorm\n",
        "# Note: loudnorm single-pass is fine for most cases; if you want perfect EBU, do 2-pass.\n",
        "fc = (\n",
        "    f\"[0:a][1:a]sidechaincompress=\"\n",
        "    f\"threshold={duck_threshold}:ratio={duck_ratio}:attack={duck_attack_ms}:release={duck_release_ms}\"\n",
        "    f\"[ducked];\"\n",
        "    f\"[ducked][1:a]amix=inputs=2:duration=longest:dropout_transition=0,\"\n",
        "    f\"loudnorm=I=-16:TP=-1.5:LRA=11\"\n",
        "    f\"[mix]\"\n",
        ")\n",
        "\n",
        "run([\n",
        "    \"ffmpeg\",\"-y\",\"-loglevel\",\"error\",\n",
        "    \"-i\", str(BKG_PREP),\n",
        "    \"-i\", str(VOC_PREP),\n",
        "    \"-filter_complex\", fc,\n",
        "    \"-map\", \"[mix]\",\n",
        "    str(FINAL_WAV)\n",
        "])\n",
        "\n",
        "step2_paths[\"final_mix_wav\"] = str(FINAL_WAV)\n",
        "print(\"Final mix saved:\", FINAL_WAV)\n",
        "print(\"Saved in step2_paths['final_mix_wav']\")\n"
      ],
      "metadata": {
        "id": "ub0WOfYF1GCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix Arabic RTL in SRT (handle mixed EN/AR)\n",
        "from pathlib import Path\n",
        "import re\n",
        "import pysrt\n",
        "\n",
        "in_path = Path(step2_paths.get(\"srt_ar_dub\",\"\"))\n",
        "if not in_path.exists():\n",
        "    raise FileNotFoundError(f\"Missing srt_ar_dub: {in_path}\")\n",
        "\n",
        "out_path = in_path.with_name(in_path.stem + \"_rtl_fixed.srt\")\n",
        "\n",
        "# Direction marks\n",
        "RLM = \"\\u200F\"   # Right-to-Left Mark\n",
        "LRM = \"\\u200E\"   # Left-to-Right Mark\n",
        "FSI = \"\\u2068\"   # First Strong Isolate\n",
        "PDI = \"\\u2069\"   # Pop Directional Isolate\n",
        "\n",
        "# Latin-ish tokens (English words, acronyms, numbers, urls)\n",
        "latin = re.compile(r\"([A-Za-z0-9][A-Za-z0-9._:/@#\\-+]*[A-Za-z0-9]?)\")\n",
        "\n",
        "def fix_line(t: str) -> str:\n",
        "    t = (t or \"\").strip()\n",
        "    if not t:\n",
        "        return t\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "\n",
        "    # wrap any Latin token as isolated LTR\n",
        "    t = latin.sub(lambda m: f\"{FSI}{LRM}{m.group(1)}{PDI}\", t)\n",
        "\n",
        "    # force the line overall to RTL\n",
        "    return RLM + t\n",
        "\n",
        "subs = pysrt.open(str(in_path), encoding=\"utf-8\")\n",
        "for s in subs:\n",
        "    lines = (s.text or \"\").splitlines()\n",
        "    s.text = \"\\n\".join(fix_line(x) for x in lines)\n",
        "\n",
        "subs.save(str(out_path), encoding=\"utf-8\")\n",
        "step2_paths[\"srt_ar_dub_rtl_fixed\"] = str(out_path)\n",
        "\n",
        "print(\"✅ Saved:\", out_path)\n",
        "print(\"Use this:\", step2_paths[\"srt_ar_dub_rtl_fixed\"])\n"
      ],
      "metadata": {
        "id": "bgvGeplc2KH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.3) Render Final Dubbed Video (MP4) { display-mode: \"form\" }\n",
        "\n",
        "add_soft_subtitles = True #@param {type:\"boolean\"}  # attach Arabic SRT as soft subs\n",
        "use_ar_srt = True #@param {type:\"boolean\"}          # attach your arabic dub srt (not the English one)\n",
        "\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "def run(cmd):\n",
        "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if p.returncode != 0:\n",
        "        print(\"CMD:\", \" \".join(map(str, cmd)))\n",
        "        print(\"STDERR tail:\\n\", (p.stderr or \"\")[-3000:])\n",
        "        raise RuntimeError(\"Command failed\")\n",
        "    return p\n",
        "\n",
        "# locate video without audio\n",
        "video_no_audio = None\n",
        "if \"outputs\" in globals() and isinstance(outputs, dict) and outputs.get(\"video_no_audio\"):\n",
        "    video_no_audio = Path(outputs[\"video_no_audio\"])\n",
        "else:\n",
        "    # fallback: search in extract dir\n",
        "    cands = sorted((DIRS[\"extract\"]).glob(\"*_no_audio.mp4\"))\n",
        "    video_no_audio = cands[-1] if cands else None\n",
        "\n",
        "if not video_no_audio or not video_no_audio.exists():\n",
        "    raise FileNotFoundError(\"video_no_audio not found. Run Step 1.2 first.\")\n",
        "\n",
        "FINAL_WAV = Path(step2_paths.get(\"final_mix_wav\",\"\"))\n",
        "if not FINAL_WAV.exists():\n",
        "    raise FileNotFoundError(\"final_mix_wav not found. Run Step 4.2 first.\")\n",
        "\n",
        "out_dir = DIRS[\"output\"]\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "video_stem = Path(INPUT_VIDEO).stem\n",
        "OUT_MP4 = out_dir / f\"{video_stem}_dubbed.mp4\"\n",
        "\n",
        "cmd = [\n",
        "    \"ffmpeg\",\"-y\",\n",
        "    \"-i\", str(video_no_audio),\n",
        "    \"-i\", str(FINAL_WAV),\n",
        "]\n",
        "\n",
        "maps = [\"-map\",\"0:v:0\",\"-map\",\"1:a:0\"]\n",
        "\n",
        "# optional subtitles\n",
        "if add_soft_subtitles:\n",
        "    srt_path = None\n",
        "    if use_ar_srt:\n",
        "        srt_path = Path(step2_paths.get(\"srt_ar_dub_rtl_fixed\", step2_paths.get(\"srt_ar_dub\",\"\")))\n",
        "    else:\n",
        "        srt_path = Path(step2_paths.get(\"asr_srt_en\",\"\"))\n",
        "\n",
        "    if srt_path and srt_path.exists():\n",
        "        cmd += [\"-i\", str(srt_path)]\n",
        "        maps += [\"-map\",\"2:s:0\"]\n",
        "    else:\n",
        "        print(\"Warning: subtitle file not found; continuing without subtitles.\")\n",
        "\n",
        "cmd += maps\n",
        "\n",
        "cmd += [\n",
        "    \"-c:v\",\"copy\",\n",
        "    \"-c:a\",\"aac\",\"-b:a\",\"192k\",\n",
        "]\n",
        "\n",
        "if add_soft_subtitles:\n",
        "    # mov_text works in mp4; if you see Arabic rendering issues, export MKV instead.\n",
        "    cmd += [\"-c:s\",\"mov_text\"]\n",
        "\n",
        "cmd += [\"-shortest\", str(OUT_MP4)]\n",
        "\n",
        "run(cmd)\n",
        "\n",
        "step2_paths[\"final_video_mp4\"] = str(OUT_MP4)\n",
        "print(\"Final video:\", OUT_MP4)\n",
        "print(\"Saved in step2_paths['final_video_mp4']\")\n"
      ],
      "metadata": {
        "id": "SaxVsBe_1ZBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.4) Copy to Drive + Download { display-mode: \"form\" }\n",
        "\n",
        "copy_to_drive = True #@param {type:\"boolean\"}\n",
        "auto_download = True #@param {type:\"boolean\"}\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "final_video = Path(step2_paths.get(\"final_video_mp4\",\"\"))\n",
        "final_mix   = Path(step2_paths.get(\"final_mix_wav\",\"\"))\n",
        "ar_srt = Path(step2_paths.get(\"srt_ar_dub_rtl_fixed\", step2_paths.get(\"srt_ar_dub\",\"\")))\n",
        "\n",
        "print(\"Artifacts:\")\n",
        "print(\" - video:\", final_video)\n",
        "print(\" - mix  :\", final_mix)\n",
        "print(\" - srt  :\", ar_srt)\n",
        "\n",
        "if copy_to_drive:\n",
        "    try:\n",
        "        drive_upload_path\n",
        "    except NameError:\n",
        "        drive_upload_path = Path(\"/content/gdrive/MyDrive\") / \"dub_project\"\n",
        "        drive_upload_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for f in [final_video, final_mix, ar_srt]:\n",
        "        if f.exists():\n",
        "            shutil.copy(str(f), str(drive_upload_path / f.name))\n",
        "            print(\"Copied:\", drive_upload_path / f.name)\n",
        "\n",
        "if auto_download:\n",
        "    from google.colab import files\n",
        "    if final_video.exists(): files.download(str(final_video))\n",
        "    if ar_srt.exists(): files.download(str(ar_srt))\n"
      ],
      "metadata": {
        "id": "NvhX0uT81bxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g0Q17kwd_-of"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}